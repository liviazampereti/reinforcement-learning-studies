import numpy as np
import matplotlib.pyplot as plt

from section_4.Maze import Maze
from section_4.Maze import plot_policy, plot_action_values, test_agent

from tqdm import tqdm

from random import randrange

env = Maze()

env.reset()
frame = env.render(mode='rgb_array')
plt.axis('off')
plt.imshow(frame)
plt.show()

action_values = np.zeros((5,5,4))
plot_action_values(action_values)

def policy(state, epsilon = 0.2):
    if np.random.random() < epsilon:
        return np.random.choice(4)
    else:
        av = action_values[state]
        return np.random.choice(np.flatnonzero(av == av.max()))
    
action = policy((0,0), epsilon = 0.5)
print(f"Action takes in state (0,0): {action}")
plot_policy(action_values, frame)

def on_policy_mc_control(policy, action_values, episode, gamma=0.99, epsilon=0.2):
    sa_returns = {}

    for episode in tqdm(range(1, episode+1)):

        #state = env.reset()

        state = (randrange(5), randrange(5))

        done = False
        transitions = []
        while not done:
            action = policy(state, epsilon)
            next_state, reward, done, _ = env.step(action)
            transitions.append([state, action, reward])
            state = next_state

        G = 0
        for state_t, action_t, reward_t in reversed(transitions):
            G = reward_t + gamma * G

            if not (state_t, action_t) in sa_returns:
                sa_returns[(state_t, action_t)] = []
            sa_returns[(state_t, action_t)].append(G)
            action_values[state_t][action_t] = np.mean(sa_returns[(state_t, action_t)])

on_policy_mc_control(policy, action_values, episode=500000)
plot_action_values(action_values)
plot_policy(action_values, frame)
test_agent(env, policy)